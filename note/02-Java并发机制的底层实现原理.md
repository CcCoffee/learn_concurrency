## 初识volatile

Java语言规范第3版中对volatile的定义如下：Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过排他锁单独获得这个变量。
这个概念听起来有些抽象，我们先看下面一个示例：

```java
package com.zwx.concurrent;

public class VolatileDemo {
    public static boolean finishFlag = false;

    public static void main(String[] args) throws InterruptedException {
        new Thread(()->{
            int i = 0;
            while (!finishFlag){
                i++;
            }
        },"t1").start();
        Thread.sleep(1000);//确保t1先进入while循环后主线程才修改finishFlag
        finishFlag = true;
    }
}
```

这里运行之后他t1线程中的while循环是一直停不下来的，因为我们是在主线程修改了finishFlag的值，而此值对t1线程不可见，如果我们把变量finishFlag加上volatile修饰:

```java
public static volatile boolean finishFlag = false;
```

这时候再去运行就会发现while循环很快就可以停下来了。
从这个例子中我们可以知道**volatile可以解决线程间变量可见性问题**。可见性的意思是当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。

## volatile如何保证可见性

利用工具hsdis，打印出汇编指令，可以发现，加了volatile修饰之后打印出来的汇编指令多了下面一行：

<img src="02-Java并发机制的底层实现原理.assets/image-20200528081558431.png" alt="image-20200528081558431" style="zoom:100%;" />

lock是一种控制指令，在多处理器环境下，lock 汇编指令可以基于总线锁或者缓存锁的机制来达到可见性的一个效果。

volatile关键字的作用是保证变量在多线程之间的可见性，它是java.util.concurrent包的核心，没有volatile就没有这么多的并发类给我们使用。

本文详细解读一下volatile关键字如何保证变量在多线程之间的可见性，在此之前，有必要讲解一下CPU缓存的相关知识，掌握这部分知识一定会让我们更好地理解volatile的原理，从而更好、更正确地地使用volatile关键字。

## 1. 可见性的本质 - 硬件层面

### CPU 高速缓存

下图简单的展示了最简单的高速缓存的配置，数据的读取和存储都经过高速缓存，CPU核心与高速缓存有一条特殊的快速通道；主存与高速缓存都连在系统总线上（BUS）这条总线同时还用于其他组件的通信：
![image-20200527001429789](02-Java并发机制的底层实现原理.assets/image-20200527001429789.png)

在高速缓存出现后不久，系统变得越来越复杂，高速缓存与主存之间的速度差异被拉大，直到加入了另一级缓存，新加入的这级缓存比第一缓存更大，但是更慢，而且经济上不合适，所以有了二级缓存，甚至有些系统已经拥有了三级缓存，于是就演变成了多级缓存，如下图：
![image-20200527001455751](02-Java并发机制的底层实现原理.assets/image-20200527001455751.png)

#### 为什么需要CPU cache

**CPU缓存的出现主要是为了解决CPU运算速度与内存读写速度不匹配的矛盾**，因为CPU运算速度要比内存读写速度快得多，举个例子：

- 一次主内存的访问通常在几十到几百个时钟周期
- 一次L1高速缓存的读写只需要1~2个时钟周期
- 一次L2高速缓存的读写也只需要数十个时钟周期

这种访问速度的显著差异，导致CPU可能会花费很长时间等待数据到来或把数据写入内存。

基于此，现在CPU大多数情况下读写都不会直接访问内存（CPU都没有连接到内存的管脚），取而代之的是CPU缓存，CPU缓存是位于CPU与内存之间的临时存储器，它的容量比内存小得多但是交换速度却比内存快得多。而缓存中的数据是内存中的一小部分数据，但这一小部分是短时间内CPU即将访问的，当CPU调用大量数据时，就可先从缓存中读取，从而加快读取速度。

按照读取顺序与CPU结合的紧密程度，CPU缓存可分为：

- 一级缓存：简称L1 Cache，位于CPU内核的旁边，是与CPU结合最为紧密的CPU缓存
- 二级缓存：简称L2 Cache，分内部和外部两种芯片，内部芯片二级缓存运行速度与主频相同，外部芯片二级缓存运行速度则只有主频的一半
- 三级缓存：简称L3 Cache，部分高端CPU才有

每一级缓存中所存储的数据全部都是下一级缓存中的一部分，这三种缓存的技术难度和制造成本是相对递减的，所以其容量也相对递增。

缓存的容量远远小于主存，因此出现缓存不命中的情况在所难免，既然缓存不能包含CPU所需要的所有数据，那么缓存的存在真的有意义吗？

CPU cache是肯定有它存在的意义的，至于CPU cache有什么意义，那就要看一下它的局部性原理了：

> 1.时间局部性：如果某个数据被访问，那么在不久的将来它很可能再次被访问
> 2.空间局部性：如果某个数据被访问，那么与它相邻的数据很快也可能被访问

当CPU要读取一个数据时，首先从一级缓存中查找，如果没有再从二级缓存中查找，如果还是没有再从三级缓存中或内存中查找。一般来说每级缓存的命中率大概都有80%左右，也就是说全部数据量的80%都可以在一级缓存中找到，只剩下20%的总数据量才需要从二级缓存、三级缓存或内存中读取。

### CPU 多核缓存架构

<img src="02-Java并发机制的底层实现原理.assets/image-20200527081147047.png" alt="image-20200527081147047" style="zoom:50%;" />

当系统运行时，CPU执行计算的过程如下：

1. **程序以及数据**被加载到主内存
2. **指令和数据**被加载到CPU缓存
3. CPU执行指令，把结果写到高速缓存
4. 高速缓存中的数据写回主内存

如果服务器是单核CPU，那么这些步骤不会有任何的问题，但是如果服务器是多核CPU，那么问题来了，以Intel Core i7处理器的高速缓存概念模型为例（图片摘自《深入理解计算机系统》）：

![image-20200530093840816](02-Java并发机制的底层实现原理.assets/image-20200530093840816.png)

试想下面一种情况：

1. 核0读取了一个字节，根据局部性原理，它**相邻的字节同样被被读入核0的缓存**
2. 核3做了上面同样的工作，这样核0与核3的缓存拥有同样的数据
3. 核0修改了那个字节，被修改后，那个字节被写回核0的缓存，但是该信息并没有写回主存
4. 核3访问该字节，由于核0并未将数据写回主存，数据不同步

再举个例子：

有两个线程对变量x=1进行加1操作，两个线程都会将x赋值到各自的CPU 缓存中去，这样的过程是如何找数据的呢？

* 首先CPU去寄存器找变量x所在的地址，如果没有就去缓存L1中找，如果L1找到就赋值到寄存器里面去进行计算。L1 的x也是从主内存拷贝过去的
* 如果L1没有x，就继续往L2找，没有找L3
* 如果L1，L2，L3都没有缓存x的地址，最后需要去主内存中找
* 找到之后依次逐级往上复制，L3，L2，L1，再到寄存器。
* 通过控制器单元发出的指令去进行算术运算，如果两个线程同时进行+1计算的话，x在CPU缓存中就会有两个副本，x都等于2
* CPU缓存同步回内存，内存里面x变量也等于2，由于x是共享变量，进行了两次+1操作，但是最后主存是2不是3。这就是数据不一致。

为了解决这个问题，CPU制造商制定了一个规则：**当一个CPU修改缓存中的字节时，服务器中其他CPU会被通知，它们的缓存将视为无效**。于是，在上面的情况下，核3发现自己的缓存中数据已无效，核0将立即把自己的数据写回主存，然后核3重新读取该数据。



```java
/**
 * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7048693.html
 */
public class LazySingleton {

    private static volatile LazySingleton instance = null;

    public static LazySingleton getInstance() {
        if (instance == null) {
            instance = new LazySingleton();
        }

        return instance;
    }

    public static void main(String[] args) {
        LazySingleton.getInstance();
    }
}
```

访问[hsdis工具路径](https://sourceforge.net/projects/fcml/files/fcml-1.1.1/hsdis-1.1.1-win32-amd64.zip/download)可直接下载hsdis工具，下载完毕之后解压，将hsdis-amd64.dll与hsdis-amd64.lib两个文件放在%JAVA_HOME%\jre\bin\server路径下即可，如下图：

![image-20200530100204329](02-Java并发机制的底层实现原理.assets/image-20200530100204329.png)

然后跑main函数，跑main函数之前，加入如下虚拟机参数：

```bash
-server -Xcomp -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly -XX:CompileCommand=compileonly,*LazySingleton.getInstance
```

运行main函数即可，代码生成的汇编指令，定位到59、60两行：

```assembly
0x0000000002931351: lock add dword ptr [rsp],0h  ;*putstatic instance
                                                ; - org.xrq.test.design.singleton.LazySingleton::getInstance@13 (line 14)
```

之所以定位到这两行是因为这里结尾写明了line 14，line 14即volatile变量instance赋值的地方。后面的add dword ptr [rsp],0h都是正常的汇编语句，意思是将双字节的栈指针寄存器+0，这里的关键就是add前面的lock指令，后面详细分析一下lock指令的作用和为什么加上lock指令后就能保证volatile关键字的内存可见性。

### lock指令

反复思考IA-32手册对lock指令作用的这几段描述，可以得出lock指令的几个作用：

1. 锁总线，其它CPU对内存的读写请求都会被阻塞，直到锁释放，不过实际后来的处理器都采用锁缓存替代锁总线，因为锁总线的开销比较大，锁总线期间其他CPU没法访问内存
2. lock后的写操作会回写已修改的数据，同时让其它CPU相关缓存行失效，从而重新从主存中加载最新的数据
3. 不是内存屏障却能完成类似内存屏障的功能，阻止屏障两遍的指令重排序

（1）中写了由于效率问题，实际后来的处理器都采用锁缓存来替代锁总线，这种场景下多缓存的数据一致是通过缓存一致性协议来保证的，我们来看一下什么是缓存一致性协议。 

### 总线锁与缓存锁

为了控制数据的一致性，CPU设计了两种方式：

#### 总线锁

总线锁，简单来说就是，在多CPU下，当其中一个处理器要对共享内存进行操作的时候，在总线上发出一个 LOCK#信号，这个信号使得其他处理器无法通过总线来访问到共享内存中的数据，总线锁定把 CPU 和内存之间的通信锁住了(CPU和内存之间通过总线进行通讯)，这使得锁定期间，其他处理器不能操作其他内存地址的数据。然而这种做法的代价显然太大，那么如何优化呢？优化的办法就是降低锁的粒度，所以CPU就引入了缓存锁。

#### 缓存锁

缓存锁的核心机制是基于缓存一致性协议来实现的，一个处理器的缓存回写到内存会导致其他处理器的缓存无效，IA-32处理器和Intel 64处理器使用MESI实现缓存一致性协议(注意，缓存一致性协议不仅仅是通过MESI实现的，不同处理器实现了不同的缓存一致性协议)。大多数时候采用这种方式，但是由于内部工作原理它也有一些场景是解决不了的，这时候还是会用到总线加锁的方式。

> 对于Pentinum4、Intel Xeon以及P6系列处理器，如果被访问的内存区域是在处理器内部进行高速缓存的，那么通常不发出LOCK#信号

### 缓存一致性协议 MESI

#### 缓存行的概念

缓存是分段（line）的，一个段对应一块存储空间，我们称之为缓存行，它是CPU缓存中可分配的最小存储单元，大小32字节、64字节、128字节不等，这与CPU架构有关，通常来说是64字节。当CPU看到一条读取内存的指令时，它会把内存地址传递给一级数据缓存，一级数据缓存会检查它是否有这个内存地址对应的缓存段，如果没有就把整个缓存段从内存（或更高一级的缓存）中加载进来。注意，这里说的是一次加载整个缓存段，这就是上面提过的局部性原理。

#### 缓存一致性协议

上面说了，LOCK#会锁总线，实际上这不现实，因为锁总线效率太低了。因此最好能做到：使用多组缓存，但是它们的行为看起来只有一组缓存那样。缓存一致性协议就是为了做到这一点而设计的，就像名称所暗示的那样，**这类协议就是要使多组缓存的内容保持一致**。

多级缓存-缓存一致性（MESI），MESI是一个协议，这协议用于保证多个CPU cache之间缓存共享数据的一致性。它定义了CacheLine的四种数据状态，而CPU对cache的四种操作可能会产生不一致的状态。因此缓存控制器监听到本地操作与远程操作的时候需要对地址一致的CacheLine状态做出一定的修改，从而保证数据在多个cache之间流转的一致性。

在 MESI 协议中，每个缓存的缓存控制器不仅知道自己的读写操作，而且也监听(snoop)其它CPU的读写操作。
对于 MESI 协议，从 CPU 读写角度来说会遵循以下原则：

* **CPU读请求**：缓存处于 M、E、S 状态都可以被读取，I 状态CPU 只能从主存中读取数据
* **CPU写请求**：缓存处于 M、E 状态才可以被写。对于S状态的写，需要将其他CPU中缓存行置为无效才行。

#### 嗅探（snooping）协议

缓存一致性协议有多种，但是日常处理的大多数计算机设备都属于"嗅探（snooping）"协议，它的基本思想是：

所有内存的传输都发生在一条共享的总线上，而所有的处理器都能看到这条总线：缓存本身是独立的，但是内存是共享资源，**所有的内存访问都要经过仲裁（同一个指令周期中，只有一个CPU缓存可以读写内存）**。

CPU缓存不仅仅在做内存传输的时候才与总线打交道，而是不停在嗅探总线上发生的数据交换，跟踪其他缓存在做什么。所以当一个缓存代表它所属的处理器去读写内存时，其它处理器都会得到通知，它们以此来使自己的缓存保持同步。只要某个处理器一写内存，其它处理器马上知道这块内存在它们的缓存段中已失效。

#### CacheLine的四种状态

MESI协议是当前最主流的缓存一致性协议，在MESI协议中，每个缓存行有4个状态，可用2个bit表示，它们分别是：

| 状态                      | 描述                                                         | 监听任务                                                     |
| :------------------------ | :----------------------------------------------------------- | :----------------------------------------------------------- |
| M 修改（Modified）        | 该Cache line有效，数据被本CPU核心修改了，和内存中的数据不一致，数据只存在于本地的Cache line中。 | 缓存行必须时刻监听所有试图读该缓存行相对应主存的操作，这种操作必须在缓存将该缓存行写回主存并将状态变成S（共享）状态之前被延迟执行。 |
| E 独享、互斥（Exclusive） | 该Cache line有效，数据和内存中的数据一致，数据只存在于本地的Cache line中。 | 缓存行也必须时刻监听其他缓存读主存中该缓存行的操作，一旦有这种操作，该缓存行需要变成S（共享）状态 |
| S 共享（Shared）          | 该Cache line有效，数据和内存中的数据一致，数据存在于很多Cache中 | 缓存行也必须时刻监听其他缓存使该缓存行无效或者独享该缓存行的请求，并将该缓存行变成无效（Invalid） |
| I 无效（Invalid）         | 该 Cache line 无效                                           | 无                                                           |

> 更详细的解释：
>
> - M: Modified 修改，指的是该缓存行只被缓存在该CPU的缓存中，并且是被修改过的，因此他与主存中的数据是不一致的， 该缓存行中的数据需要在未来的某个时间点（允许其他CPU读取主存相应中的内容之前）写回主存，而当数据被写回主存之后，该缓存行的状态会变成E（独享）
> - E：Exclusive 独享 缓存行只被缓存在该CPU的缓存中，是未被修改过的，与主存的数据是一致的，可以在任何时刻当有其他CPU读取该内存时，变成S（共享）状态，同样的当CPU修改该缓存行的内容时，会变成M（被修改）的状态
> - S：Share 共享，当前CPU和其他CPU中都有共同数据，并且和主存中的数据一致；意味着该缓存行可能会被多个CPU进行缓存，并且各缓存中的数据与主存数据是一致的，当有一个CPU修改该缓存行时，在其他CPU中的该缓存行是可以被作废的，变成I(无效的) 状态
> - I：Invalid 无效的，代表这个缓存是无效的，可能是有其他CPU修改了该缓存行；数据应该从主存中获取，其他CPU中可能有数据也可能无数据，当前CPU中的数据和主存被认为是不一致的；对于invalid而言，在MESI协议中采取的是写失效（write invalidate）。

这里的I、S和M状态已经有了对应的概念：失效/未载入、干净以及脏的缓存段。所以这里新的知识点只有E状态，代表独占式访问，这个状态解决了"在我们开始修改某块内存之前，我们需要告诉其它处理器"这一问题：只有当缓存行处于E或者M状态时，处理器才能去写它，也就是说只有在这两种状态下，处理器是独占这个缓存行的。当处理器想写某个缓存行时，如果它没有独占权，它必须先发送一条"我要独占权"的请求给总线，**这会通知其它处理器把它们拥有的同一缓存段的拷贝失效**（如果有）。只有在获得独占权后，处理器才能开始修改数据----并且此时这个处理器知道，这个缓存行只有一份拷贝，在我自己的缓存里，所以不会有任何冲突。

反之，如果有其它处理器想读取这个缓存行（马上能知道，因为一直在嗅探总线），独占或已修改的缓存行必须先回到"共享"状态。如果是已修改的缓存行，那么还要先把内容回写到内存中。

MESI示意图：
![image-20200526234701239](02-Java并发机制的底层实现原理.assets/image-20200526234701239.png)

#### 引起数据状态转换的四种操作

CacheLine有四种数据状态（MESI），而引起数据状态转换的CPU cache操作也有四种：

- local read：读本地缓存中的数据
- local write：将数据写到本地缓存里面
- remote read：将内（主）存中的数据读取到缓存中来
- remote write：将缓存中的数据写回到主存里面去

因此要完整的理解MESI这个协议，就需要把这16种状态转换的情况理解清楚，状态之间的相互转换关系，可以使用下图进行表示：
![img](02-Java并发机制的底层实现原理.assets/1014100-20180613225011165-302241278.png)

状态之间的相互转换关系也可以使用下表进行表示。

![image-20200526234626132](02-Java并发机制的底层实现原理.assets/image-20200526234626132.png)

#### 分析案例

假设有三个CPU1、CPU2、CPU3，对应三个缓存分别时cache1、cache2和cache3。在主内存中定义了x变量的值为1。对 x 进行加1操作。

**1. CPU1 要去内存读取变量 x**

<img src="02-Java并发机制的底层实现原理.assets/image-20200527084909659.png" alt="image-20200527084909659" style="zoom:50%;" />

CPU1 要去内存读取变量 x，并且赋值到CPU1的cache1中。首先向总线发出一条要读取x的消息，CPU1就会从内存里面读取x的变量，并且会把缓存行的状态设为 E 独享状态。为了保证缓存的一致性，在汇编指令里面它会加一个 lock的信号，就会触发MESI的协议，CPU1它时刻就会去监听这个总线当中是不是有其他的CPU来操作或试图去读取这一块内存。假设CPU1缓存还没有回写的时候，CPU2就发出一条指令去读取x，CPU1就会感知到有其他的CPU在读取变量x，这就是**总线嗅探机制**。检测到这个地址冲突之后CPU1就会对缓存的数据进行相应的响应，会把这个变量x缓存行的状态置为共享S的状态。此时X的这个变量就存在于两个CPU缓存里面的副本，并且他们的状态都是S。

| CPU  | 状态 |
| ---- | ---- |
| 1    | M    |
| 2    | S    |

**2. CPU1 对 x 进行+1操作**

<img src="02-Java并发机制的底层实现原理.assets/image-20200527085006323.png" alt="image-20200527085006323" style="zoom:50%;" />

1. CPU1向总线发出一条消息，我要去修改内存中x变量了。这时候首先要锁住这个缓存行，并且将S 状态置为M状态。
2. CPU2会嗅探到CPU1要去修改x变量，CPU2就将数据 x 由 S 状态置为无效 I 状态。
3. CPU1此时就可以把x修改为2了。
4. CPU1修改完后就可以把数据同步到主内存。这不是实时同步回内存的。

| CPU  | 状态   |
| ---- | ------ |
| 1    | S -> M |
| 2    | S -> I |

**3. CPU1 同步数据到主内存**

<img src="02-Java并发机制的底层实现原理.assets/image-20200527091809377.png" alt="image-20200527091809377" style="zoom:50%;" />

1. CPU2向总线发出一个读取主内存x的消息的时候，CPU1就会感知到这个信息，就会把修改后的信息同步回内存，此时CPU1还是处于E的状态
2. 然后CPU2读取内存中 x 再同步到cache2中去。CPU1将 x 由 E 置为S状态，CPU2将x由 I 状态置为S状态。

| CPU  | 状态   |
| ---- | ------ |
| 1    | M -> S |
| 2    | I -> S |

#### 多个CPU同时修改变量

两个CPU都读到数据之后同时都发出对M的修改信息呢？不会出现这种情况，因为在一个指令周期内会进行一个裁决，属于硬件方面的知识，如果裁决成功的就成为M状态，裁决失败的就成为 I 状态。就不存在由多个M修改的状态。裁决失败后是否需要继续读这个数据就取决于程序操作指令是否要求再去读这个数据。

> **所有的内存访问都要经过仲裁（同一个指令周期中，只有一个CPU缓存可以读写内存）**。

#### 缓存一致性协议不起作用的情况

1. 变量比较大，一个缓存行存不下，比如跨了两个缓存行，这时缓存一致性协议就锁不住缓存了，只能通过总线加锁的方式
2. CPU本身不支持MESI协议

#### 总结

在一个典型的多核系统中，每一个核都会有自己的缓存来共享主存总线，每个相应的CPU会发出读写（I/O）请求，而

* **缓存的目的是为了减少CPU读写共享主存的次数**。
* 一个缓存除了在 Invalid 状态之外，都可以满足CPU的读请求。

一个写请求只有在该缓存行是M状态，或者E状态的时候才能够被执行。如果当前状态是处在S状态的时候，它必须先将其他CPU缓存中的对应的缓存行变成无效的（Invalid）状态，这个操作通常作用于广播的方式来完成。这个时候它既不允许不同的CPU同时修改同一个缓存行，即使修改该缓存行不同位置的数据也是不允许的，这里主要解决的是缓存一致性的问题。

一个处于M状态的缓存行它必须时刻监听所有试图读该缓存行相对应的主存的操作，这种操作必须在缓存将该缓存行写回主存并将状态变成S状态之前被延迟执行。

一个处于S状态的缓存行也必须监听其它缓存使该缓存行无效或者独享该缓存行的请求，并将该缓存行变成无效（Invalid）。

一个处于E状态的缓存行也必须监听其它缓存读主存中该缓存行的操作，一旦有这种操作，该缓存行需要变成S状态。

因此，对于M和E两种状态而言总是精确的，他们和该缓存行的真正状态是一致的。而S状态可能是非一致的，如果一个缓存将处于S状态的缓存行作废了，而另一个缓存实际上可能已经独享了该缓存行，但是该缓存却不会将该缓存行升迁为E状态，这是因为其它缓存不会广播他们作废掉该缓存行的通知，同样由于缓存并没有保存该缓存行的copy的数量，因此（即使有这种通知）也没有办法确定自己是否已经独享了该缓存行。

从上面的意义看来E状态是一种投机性的优化：**如果一个CPU想修改一个处于S状态的缓存行，总线事务需要将所有该缓存行的copy变成invalid状态，而修改E状态的缓存不需要使用总线事务**。

### 乱序执行优化与 MESI 协议带来的问题

MESI协议虽然可以实现缓存的一致性，但是也会存在一些问题：就是各个CPU缓存行的状态是通过消息传递来进行的。如果CPU0要对一个在缓存中共享的变量进行写入，首先需要发送一个失效的消息给到其他缓存了该数据的 CPU。并且要等到他们的确认回执。CPU0在这段时间内都会处于阻塞状态。为了避免阻塞带来的资源浪费。CPU中又引入了store bufferes：
![image-20200527232620472](02-Java并发机制的底层实现原理.assets/image-20200527232620472.png)

如上图，CPU0 只需要在写入共享数据时，直接把数据写入到 store bufferes中，同时发送invalidate消息，然后继续去处理其他指令（异步，实际上表现为指令执行的乱序优化） 当收到其他所有 CPU 发送了invalidate acknowledge消息时，再将store bufferes中的数据数据存储至缓存行中，最后再从缓存行同步到主内存。但是**这种优化就会带来了可见性问题，也可以认为是CPU的乱序执行引起的或者说是指令重排序引起的**(指令重排序不仅仅在CPU层面存在，编译器层面也存在指令重排序)。

我们通过下面一个简单的示例来看一下指令重排序带来的问题。

```java
package com.zwx.concurrent;

public class ReSortDemo {
    int value;
    boolean isFinish;

    void cpu0(){
        value = 10;//S->I状态，将value写入store bufferes，通知其他CPU当前value的缓存失效
        isFinish=true;//E状态
    }
    void cpu1(){
        if (isFinish){//true
            System.out.println(value == 10);//可能为false
        }
    }
}
```

这时候理论上当isFinish为true时，value也要等于10，然而由于当value修改为10之后，发送消息通知其他CPU还没有收到响应时，当前CPU0继续执行了isFinish=true，所以就可能存在isFinsh为true时，而value并不等于10的问题。
我们想一想，其实从硬件层面很难去知道软件层面上的这种前后依赖关系，所以没有办法通过某种手段自动去解决，故而**CPU层面就提供了内存屏障(Memory Barrier，Intel称之为 Memory Fence),使得软件层面可以决定在适当的地方来插入内存屏障来禁止指令重排序。**

### CPU层面的内存屏障

CPU内存屏障主要分为以下三类：
**写屏障(Store Memory Barrier)**：告诉处理器在写屏障之前的所有已经存储在存储缓存(store bufferes)中的数据同步到主内存，简单来说就是使得写屏障之前的指令的结果对写屏障之后的读或者写是可见的。
**读屏障(Load Memory Barrier)**：处理器在读屏障之后的读操作,都在读屏障之后执行。配合写屏障，使得写屏障之前的内存更新对于读屏障之后的读操作是可见的。
**全屏障(Full Memory Barrier)**：确保屏障前的内存读写操作的结果提交到内存之后，再执行屏障后的读写操作。

这些概念听起来可能有点模糊，我们通过将上面的例子改写一下来说明：

```java
package com.zwx.concurrent;

public class ReSortDemo {
    int value;
    boolean isFinish;

    void cpu0(){
        value = 10;//S->I状态，将value写入store bufferes，通知其他CPU当前value的缓存失效
        storeMemoryBarrier();//伪代码，插入一个写屏障，使得value=10这个值强制写入主内存
        isFinish=true;//E状态
    }
    void cpu1(){
        if (isFinish){//true
            loadMemoryBarrier();//伪代码，插入一个读屏障，强制cpu1从主内存中获取最新数据
            System.out.println(value == 10);//true
        }
    }

    void storeMemoryBarrier(){//写屏障
    }
    void loadMemoryBarrier(){//读屏障
    }
}
```

通过以上内存屏障，我们就可以防止了指令重排序，得到我们预期的结果。
总的来说，**内存屏障的作用可以通过防止 CPU 对内存的乱序访问来保证共享数据在多线程并行执行下的可见性**，但是这个屏障怎么来加呢？回到最开始我们讲 volatile关键字的代码，这个关键字会生成一个 lock 的汇编指令，这个就相当于实现了一种内存屏障。接下来我们进入volatile原理分析的正题。

## 2. 可见性的本质 - JVM 层面

### Java内存模型 JMM

以上我们简单介绍了在多核并发的环境下CPU进行乱序执行优化时所带来的线程安全问题，为了保证线程安全，我们需要采取一些额外的手段去防止这种问题的发生。在JVM层面，定义了一种抽象的内存模型(JMM)来规范并控制重排序，从而解决可见性问题。通过前面的分析发现，**导致可见性问题的根本原因是缓存以及指令重排序**。 而JMM 实际上就是提供了合理的禁用缓存以及禁止重排序的方法。所以**JMM最核心的价值在于解决可见性和有序性**。

不过在介绍如何采用实际手段解决这种问题之前，我们先来看看Java虚拟机是如何解决这种问题的：**为了屏蔽各种硬件和操作系统内存的访问差异，以实现让Java程序在各种平台下都能达到一致的并发效果，所以Java虚拟机规范中定义了Java内存模型（Java Memory Model简称JMM）。**

JMM属于语言级别的抽象内存模型，可以简单**理解为对硬件模型的抽象**，它定义了共享内存中多线程程序读写操作的行为规范，通过这些规则来规范对内存的读写操作从而保证指令的正确性，它解决了CPU 多级缓存、处理器优化、指令重排序导致的内存访问问题，保证了并发场景下的可见性。
需要注意的是，JMM并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序，也就是说**在JMM中，也会存在缓存一致性问题和指令重排序问题。只是JMM把底层的问题抽象到JVM层面，再基于CPU层面提供的内存屏障指令，以及限制编译器的重排序来解决并发问题**。

![image-20200527001649747](02-Java并发机制的底层实现原理.assets/image-20200527001649747.png)

在明确了Java内存模型是做什么的之后，我们来看一下其中内存分配的两个概念

- Head（堆）：java里的堆是一个运行时的数据区，堆是由垃圾回收机制来负责的。堆的优势是可以动态地分配内存大小，生存期也不必事先告诉编译器，因为它是在运行时动态分配内存的，而且Java的垃圾回收机制也会自动的收走那些不再使用的数据。但是它也有缺点，由于是运行时动态分配内存，因此它的存取速度相对要慢一些。
- Stack（栈）：栈的优势是存取速度比堆要快，仅次于计算机里的寄存器，栈的数据是可以共享的。而栈的缺点则是存在栈中的数据的大小以及生存期必须是确定的，缺乏一些灵活性，所以栈中主要用来存储一些基本数据类型的变量，比如：int，short，long，byte，double，float，boolean，char以及对象句柄等。

Java内存模型要求调用栈和本地变量存放在线程栈（Thread Stack）上，而对象则存放在堆上。一个本地变量也可能是指向一个对象的引用，这种情况下这个保存对象引用的本地变量是存放在线程栈上的，但是对象本身则是存放在堆上的。

**一个对象可能包含方法，而这些方法可能包含着本地变量，这些本地变量仍然是存放在线程栈上的。即使这些方法所属的对象是存放在堆上的。**一个对象的成员变量，可能会随着所属对象而存放在堆上，不管这个成员变量是原始类型还是引用类型。静态成员变量则是随着类的定义一起存放在堆上。

存放在堆上的对象，可以被持有这个对象的引用的线程访问。当一个线程可以访问某个对象时，它也可以访问该对象的成员变量。**如果两个线程同时调用同一个对象上的同一个方法，那么它们都将会访问这个方法中的成员变量，但是每一个线程都拥有这个成员变量的私有拷贝。**

#### 硬件内存架构

现代硬件内存模型与Java内存模型有一些不同。理解内存模型架构以及Java内存模型如何与它协同工作也是非常重要的。这部分描述了通用的硬件内存架构，下面的部分将会描述Java内存是如何与它“联手”工作的。

下图简单展示了现代计算机硬件内存架构：
![Java并发编程（1）-并发基础](02-Java并发机制的底层实现原理.assets/image-20200527001707363.png)

> 一些CPU还有多层缓存，但这些对理解Java内存模型如何和内存交互不是那么重要。只要知道CPU中可以有一个缓存层就可以了。

运作原理：通常情况下，当一个CPU需要读取主存时，它会将主存的部分读到CPU缓存中。它甚至可能将缓存中的部分内容读到它的内部寄存器中，然后在寄存器中执行操作。当CPU需要将结果写回到主存中去时，它会将内部寄存器的值刷新到缓存中（E -> M 状态），然后在某个时间点（当其他CPU内核需要读取主存中该缓存行时向总线发出消息，我要读内存了。处于监听状态的该CPU将会刷新cache中的数据到主存中。M -> S）将值刷新回主存。

当CPU需要在缓存层存放一些东西的时候，存放在缓存中的内容通常会被刷新回主存。CPU缓存可以在某一时刻将数据局部写到它的内存中，和在某一时刻局部刷新它的内存。它不会再某一时刻读/写整个缓存。通常，在一个被称作“cache lines”的更小的内存块中缓存被更新。一个或者多个缓存行可能被读到缓存，一个或者多个缓存行可能再被刷新回主存。

### JMM和硬件内存架构之间的桥接

上面已经提到，Java内存模型与硬件内存架构之间存在差异。硬件内存架构没有区分线程栈和堆。对于硬件而言，所有的线程栈和堆都分布在主内存中。**部分线程栈和堆可能有时候会出现在CPU缓存中和CPU内部的寄存器中**。如下图所示：

![image-20200527001732477](02-Java并发机制的底层实现原理.assets/image-20200527001732477.png)

#### 线程和主内存的抽象关系

Java内存模型抽象结构图：
![image-20200527001754093](02-Java并发机制的底层实现原理.assets/image-20200527001754093.png)

每个线程之间的共享变量存储在JMM概念的主内存里面，**每个线程都有一个私有的本地内存(工作内存)**，本地内存是Java内存模型的一个抽象的概念，并不是真实存在的。它涵盖了缓存、写缓存区、寄存器以及其他的硬件和编译器的优化，**本地内存中存储了该线程已读或写共享变量的拷贝的一个副本。**工作内存是每个线程独占的，线程对变量的所有操作都必须在工作内存中进行，不能直接读写主内存中的变量，线程之间的共享变量值的传递都是基于主内存来完成。

从一个更低的层次来说，主内存就是硬件的内存，而为了获取更好的运行速度，虚拟机及硬件系统可能会让工作内存优先存储于寄存器和高速缓存中。

Java内存模型中的线程的工作内存（working memory）是cpu的寄存器和高速缓存的抽象描述。而JVM的静态内存存储模型（JVM内存模型）只是一种对内存的物理划分而已，它只局限在内存，而且只局限在JVM的内存。

![image-20200528085314876](02-Java并发机制的底层实现原理.assets/image-20200528085314876.png)

如果上图中的线程A和线程B要通信，必须经历两个步骤：

1. 首先线程A要把本地内存A中更新过的共享变量刷新到主内存里
2. 然后线程B再到主内存中去读取线程A更新的共享变量，这样就完成了两个线程之间的通信了

假设初始时，这3个内存中的x值都为0。线程A在执行时，把更新后的x值（假设值为1）临时存放在自己的本地内存 A中。当线程A和线程B需要通信时，线程A首先会把自己本地内存中修改后的x值刷新到主内 存中，此时主内存中的x值变为了1。随后，线程B到主内存中去读取线程A更新后的x值，此时线程B的本地内存的x值也变为了1。 从整体来看，这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。

然而，在多线程的环境下实际上会出现线程安全问题。例如我们要进行一个计数的操作：线程A在主内存中读取到了变量值为1，然后保存到本地内存A中进行累加。就在此时线程B并没有等待线程A把累加后的结果写入到主内存中再进行读取，而是在主内存中直接读取到了变量值为1，然后保存到本地内存B中进行累加。此时，两个线程之间的数据是不可见的，当两个线程同时把计算后的结果都写入到主内存中，就导致了计算结果是错误的。这种情况下，我们就需要采取一些同步的手段，确保在并发环境下，程序处理结果的准确性。**JMM通过控制主内存与每个线程的本地内存之间的交互（JVM定义了同步的八种操作），来为Java程序员提供内存可见性保证**。

> CPU硬件层面通过 MESI 缓存一致性协议中定义的 local read、local write、remote read 和 remote write四种操作修改 cache line的状态。而在抽象的 JMM 中也定义了同步的八种操作来控制共享变量的可见性（一致性问题）。

### 编译器的指令重排序

综合上面从硬件层面和JVM层面的分析，我们知道在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。

例如，在以下案例中，编译器为了尽可能地减少寄存器的读取、存储次数，会充分复用寄存器的存储值。如果没有进行重排序优化，正常的执行顺序是步骤 1\2\3，而在编译期间进行了重排序优化之后，执行的步骤有可能就变成了步骤 1/3/2 或者 2/1/3，这样就能减少一次寄存器的存取次数。

```bash
int x = 1;// 步骤 1：加载 x 变量的内存地址到寄存器中，加载 1 到寄存器中，CPU 通过 mov 指令把 1 写入到寄存器指定的内存中
boolean flag = true; // 步骤 2 加载 flag 变量的内存地址到寄存器中，加载 true 到寄存器中，CPU 通过 mov 指令把 1 写入到寄存器指定的内存中
int y = x + 1;// 步骤 3 重新加载 x 变量的内存地址到寄存器中，加载 1 到寄存器中，CPU 通过 mov 指令把 1 写入到寄存器指定的内存中
```

在 JVM 中，重排序是十分重要的一环，特别是在并发编程中。可 JVM 要是能对它们进行任意排序的话，也可能会给并发编程带来一系列的问题，其中就包括了一致性的问题。

**重排序分3种类型：**

1）编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。
2）指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。
3）内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。
从Java源代码到最终实际执行的指令序列，会分别经历下面3种重排序，如下图：

<img src="02-Java并发机制的底层实现原理.assets/image-20200528090351287.png" alt="image-20200528090351287" style="zoom:100%;" />

其中2和3属于处理器重排序(前面硬件层面已经分析过了)。而这些重排序都可能会导致可见性问题（编译器和处理器在重排序时会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序，编译器会遵守[happens-before规则和as-if-serial语义](https://blog.csdn.net/zwx900102/article/details/106320017)）。

* 对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。
* 对于处理器重排序，JMM的处理器重排序规则会要求Java编译器在生成指令序列时，插入特定类型的内存屏障（Memory Barriers，Intel称之为Memory Fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序。

JMM属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。正是因为volatile的这个特性，所以[单例模式中可以通过volatile关键字来解决双重检查锁(DCL)写法中所存在的问题](https://blog.csdn.net/zwx900102/article/details/90171066)。


### JMM定义同步的八种操作

- lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态
- unlock（解锁）：作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定
- read（读取）：作用于主内存的变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用
- load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中
- use（使用）：作用于工作内存变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用某个变量的字节码指定时就会执行这个操作
- assing（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指定时就会执行这个操作
- store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write操作能够使用
- write（写入）：作用于工作内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中

**同步规则**

- 如果要把一个变量从主内存中复制到工作内存，就需要按顺序的执行read和load操作，如果把变量从工作内存中同步回主内存中，就要按顺序的执行store和write操作。但Java内存模型只要求上述操作必须按顺序执行，而没有保证必须是连续执行。
- 不允许read和load、store和write操作之一单独出现
- 不允许一个线程丢弃它的最近assin的操作，即变量在工作内存中改变了之后必须同步到主内存中。
- 不允许一个线程无原因的（没有发生过任何assin操作）把数据从工作内存同步回主内存中
- 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assin）的变量。即就是对一个变量实施use和store操作之前，必须先执行过了load或assin操作。
- 一个变量在同一个时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。所以lock和unlock必须成对出现。
- 如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值。
- 如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量。
- 对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）

同步操作与规则：
![image-20200527001832363](02-Java并发机制的底层实现原理.assets/image-20200527001832363.png)

### JMM层面的内存屏障

首先介绍广义上的内存屏障分类：

* 写屏障(store barrier)：所有在store barrier之前的所有store指令，都要在该store barrier之前执行(这里需要将修改的值都要刷新到缓存)，并发送缓存失效的信号。所有在store barrier指令之后的store指令，都必须在store barrier之前的指令执行完后再被执行。
* 读屏障(load barrier)：所有在load barrier之前的所有load 指令，都要在该load barrier之前执行(这里需要将Invalid Queue中的消息执行完毕)。所有在load barrier指令之后的load 指令，都必须在load barrier之前的指令执行完后再被执行。
* 全屏障(Full Barrier)：所有在storeload barrier之前的store/load指令，都在该屏障之前被执行
  所有在该屏障之后的的store/load指令，都在该屏障之后被执行。

在JMM 中把内存屏障分为四类：

![image-20200528091144533](02-Java并发机制的底层实现原理.assets/image-20200528091144533.png)

IA-32架构的lock指令相当于StoreLoad Barriers，是一个“全能型”的屏障，它同时具有其他3个屏障的效果。现代的多数处理器大多支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（Buffer Fully Flush）。

在某些架构上，JVM使用的是全屏障保证volatile的语义，有些架构上则分别使用写屏障和读屏障保证volatile的语义。

### happens-before规则

happens-before表示的是前一个操作的结果对于后续操作是可见的，它是一种表达多个线程之间对于内存的可见性。所以我们可以认为在 JMM 中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作必须要存在happens-before关系。这两个操作可以是同一个线程，也可以是不同的线程。

### volatile的使用场景

volatile的使用场景必须符合以下两条规则：

1. 运算结果并不依赖变量的当前值，或者能够确保只有单一的线程来修改变量的值。
2. 变量不需要与其他的状态共同参与不变约束。

其中《Java并发编程实战》提到了规则三：在访问变量时不需要加锁(都需要使用到重量级的加锁操作了那还需要轻量级的volatile干嘛呢?!)。

典型用法：检查某个状态标记以判断是否执行相应操作，volatile变量常用作某个操作完成、发生中断或者状态的标志。

## 一致性的级别

### 严格一致性（强一致性）

所有的读写操作都按照全局时钟下的顺序执行，且任何时刻线程读取到的缓存数据都是一样的，Hashtable 就是严格一致性；

<img src="02-Java并发机制的底层实现原理.assets/image-20200614223521183.png" alt="image-20200614223521183" style="zoom:67%;" />

### 顺序一致性

多个线程的整体执行可能是无序的，但对于单个线程而言执行是有序的，要保证任何一次读都能读到最近一次写入的数据，volatile 可以阻止指令重排序，所以修饰的变量的程序属于顺序一致性；

<img src="02-Java并发机制的底层实现原理.assets/image-20200614223544017.png" alt="image-20200614223544017" style="zoom:67%;" />

### 弱一致性

不能保证任何一次读都能读到最近一次写入的数据，但能保证最终可以读到写入的数据，单个写锁 + 无锁读，就是弱一致性的一种实现。


## 总结

并发编程中有三大特性：原子性、可见性、有序性，volatile通过内存屏障禁止指令重排序，主要遵循以下三个规则：

1. 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。
2. 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。
3. 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。

为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能。为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略：

* 在每个volatile写操作的前面插入一个StoreStore屏障。
* 在每个volatile写操作的后面插入一个StoreLoad屏障。
* 在每个volatile读操作的后面插入一个LoadLoad屏障。
* 在每个volatile读操作的后面插入一个LoadStore屏障。

最后需要特别提一下原子性，Java语言规范鼓励但不强求JVM对64位的long型变量和double型变量的写操作具有原子性。当JVM在这种处理器上运行时，可能会把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行，这两个32位的写操作可能会被分配到不同的总线事务中执行，此时对这个64位变量的写操作将不具有原子性。

锁的语义决定了临界区代码的执行具有原子性。但是因为一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入，所以**即使是64位的long型和double型变量，只要它是volatile变量，对该变量的读/写就具有原子性。但是多个volatile操作或类似于i++这种复合操作，这些操作整体上不具有原子性**。针对于复合操作如i++这种，如果要保证原子性，需要通过synchronized关键字或者加其他锁来处理。

注意：在JSR-133之前的旧内存模型中，一个64位long/double型变量的读/写操作可以被拆分为两个32位的读/写操作来执行。从JSR-133内存模型开始（即从JDK5开始），仅仅只允许把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行，任意的读操作在JSR-133中都必须具有原子性（即任意读操作必须要在单个读事务中执行）。



## 并发的风险与优势

![image-20200527001849102](02-Java并发机制的底层实现原理.assets/image-20200527001849102.png)



## 参考链接

* [volatile内存屏障及实现原理分析(JMM和MESI)](https://blog.csdn.net/zwx900102/article/details/106306915/)
* [Java并发编程（1）-并发基础](https://blog.51cto.com/zero01/2094499)

* [就是要你懂Java中volatile关键字实现原理](https://www.cnblogs.com/xrq730/p/7048693.html)

